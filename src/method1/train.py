import os
import argparse
import torch
import pytorch_lightning as L
from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar, RichModelSummary
from pytorch_lightning.loggers import WandbLogger
from dotenv import load_dotenv
from pathlib import Path

# Import c√°c module c·ªßa Method 1
from dataset import NAMLDataModule
from model import VariantNAMLConfig
from lightning_module import NAMLLightningModule

# Load bi·∫øn m√¥i tr∆∞·ªùng (n·∫øu c√≥ file .env)
load_dotenv()

# T·ªëi ∆∞u h√≥a hi·ªáu nƒÉng
torch.set_float32_matmul_precision('medium')  # TƒÉng t·ªëc tr√™n GPU ƒë·ªùi m·ªõi (A100, H100, 3090...)
torch.set_num_threads(4)
os.environ["OMP_NUM_THREADS"] = "4"
os.environ["MKL_NUM_THREADS"] = "4"

# ==========================================
# C·∫§U H√åNH M·∫∂C ƒê·ªäNH
# ==========================================
DEFAULT_ROOT_DIR = "/home2/congnh/wm/processed_parquet"
DEFAULT_EMB_DIR = "/home2/congnh/wm/embedding"


def parse_args():
    parser = argparse.ArgumentParser(description="Train NAML Method 1 (VariantNAML)")

    # --- Paths ---
    parser.add_argument("--root-dir", type=str, default=DEFAULT_ROOT_DIR,
                        help="Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω (ch·ª©a train/val folders v√† article_ids.npy)")
    parser.add_argument("--embedding-dir", type=str, default=DEFAULT_EMB_DIR,
                        help="Th∆∞ m·ª•c ch·ª©a c√°c file embedding weight (.npy)")

    # --- Training Hyperparams ---
    parser.add_argument("--batch-size", type=int, default=512, help="Batch size")
    parser.add_argument("--epochs", type=int, default=10, help="S·ªë l∆∞·ª£ng epoch")
    parser.add_argument("--lr", type=float, default=1e-3, help="Learning rate")
    parser.add_argument("--weight-decay", type=float, default=1e-4, help="Weight decay")

    # --- Model Config (Optional override) ---
    parser.add_argument("--dropout", type=float, default=0.2, help="Dropout rate")

    # --- Data Config ---
    parser.add_argument("--history-len", type=int, default=30, help="ƒê·ªô d√†i l·ªãch s·ª≠ ƒë·ªçc")
    parser.add_argument("--neg-ratio", type=int, default=4, help="S·ªë l∆∞·ª£ng negative samples cho m·ªói positive")
    parser.add_argument("--num-workers", type=int, default=4, help="S·ªë worker cho DataLoader")

    # --- System ---
    parser.add_argument("--wandb-project", type=str, default="NAML-Method1", help="T√™n project tr√™n WandB")
    parser.add_argument("--wandb-name", type=str, default=None, help="T√™n run c·ª• th·ªÉ (optional)")
    parser.add_argument("--offline", action="store_true", help="Ch·∫°y ch·∫ø ƒë·ªô offline (kh√¥ng sync wandb)")

    return parser.parse_args()


def main():
    torch.autograd.set_detect_anomaly(True)
    args = parse_args()

    # Set seed ƒë·ªÉ ƒë·∫£m b·∫£o t√°i l·∫≠p k·∫øt qu·∫£
    L.seed_everything(42)

    print("=" * 50)
    print(f"üöÄ TRAINING METHOD 1: VariantNAML")
    print(f"   Data Dir:      {args.root_dir}")
    print(f"   Embedding Dir: {args.embedding_dir}")
    print(f"   Batch Size:    {args.batch_size}")
    print(f"   Learning Rate: {args.lr}")
    print("=" * 50)

    # 1. Kh·ªüi t·∫°o Config & Model
    config = VariantNAMLConfig()
    config.dropout = args.dropout  # Override t·ª´ arguments

    model = NAMLLightningModule(
        config=config,
        embedding_dir=args.embedding_dir,
        lr=args.lr,
        weight_decay=args.weight_decay
    )

    # 2. Kh·ªüi t·∫°o DataModule
    # L∆∞u √Ω: embedding_path c·ªßa DataModule tr·ªè v·ªÅ root_dir v√¨ c·∫ßn file article_ids.npy ·ªü ƒë√≥
    dm = NAMLDataModule(
        root_path=args.root_dir,
        embedding_path=args.embedding_dir,
        batch_size=args.batch_size,
        history_len=args.history_len,
        neg_ratio=args.neg_ratio,
        num_workers=args.num_workers
    )

    # 3. Logger (WandB)
    run_name = args.wandb_name if args.wandb_name else f"M1-bs{args.batch_size}-lr{args.lr}"
    wandb_logger = WandbLogger(
        project=args.wandb_project,
        name=run_name,
        log_model=False,
        mode="offline" if args.offline else "online"
    )

    # 4. Callbacks
    checkpoint_callback = ModelCheckpoint(
        dirpath="checkpoints/method1",
        filename="naml-m1-{epoch:02d}-{val/auc:.4f}",
        save_top_k=3,
        monitor="val/auc",
        mode="max",
        verbose=True,
        save_last=True
    )

    # 5. Trainer
    trainer = L.Trainer(
        accelerator="auto",
        devices="auto",
        strategy="auto",  # T·ª± ƒë·ªông d√πng DDP n·∫øu c√≥ nhi·ªÅu GPU
        logger=wandb_logger,
        callbacks=[
            checkpoint_callback,
            RichModelSummary(max_depth=2),
            RichProgressBar(refresh_rate=1)
        ],
        max_epochs=args.epochs,
        precision="16-mixed",  # Mixed precision gi√∫p train nhanh h∆°n v√† √≠t VRAM h∆°n
        gradient_clip_val=0.5,  # Ch·ªëng b√πng n·ªï gradient
        log_every_n_steps=50
    )

    # 6. B·∫Øt ƒë·∫ßu Train
    trainer.fit(model, datamodule=dm)

    print(f"\n‚úÖ Training Completed!")
    print(f"üèÜ Best Model: {checkpoint_callback.best_model_path}\n")


if __name__ == "__main__":
    main()