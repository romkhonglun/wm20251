import torch
import polars as pl
import numpy as np
import pyarrow.parquet as pq
from torch.utils.data import IterableDataset, DataLoader, get_worker_info
from pathlib import Path
import pytorch_lightning as L
import torch.distributed as dist
from itertools import islice
import os
import gc


# ==========================================
# 1. C·∫§U H√åNH & HELPER
# ==========================================
class WPMConfig:
    # WPM config c≈© kh√¥ng c√≤n d√πng cho t√≠nh score n·ªØa
    # nh∆∞ng v·∫´n gi·ªØ MAX_HISTORY ƒë·ªÉ ƒë·ªãnh h√¨nh ma tr·∫≠n
    MAX_HISTORY = 30


def build_history_matrix(history_path, articles_path, output_path):
    """
    H√†m t√≠nh to√°n Cache.
    Thay ƒë·ªïi: T√≠nh log1p(hist_time) thay v√¨ WPM Score.
    """
    print(f"üöÄ [INIT] ƒêang t·∫°o Ma tr·∫≠n L·ªãch s·ª≠ (Log1p Time)...")

    # 1. Load Data (B·ªè qua body_len v√¨ kh√¥ng c·∫ßn d√πng ƒë·ªÉ t√≠nh time score n·ªØa)
    # Tuy nhi√™n v·∫´n c·∫ßn join article n·∫øu b·∫°n mu·ªën l·ªçc b√†i vi·∫øt r√°c,
    # nh∆∞ng ·ªü ƒë√¢y t√¥i t·ªëi gi·∫£n ch·ªâ l·∫•y history.

    lf_hist = pl.scan_parquet(history_path).select([
        pl.col("user_id").cast(pl.Int32),
        pl.col("hist_ids"),
        pl.col("hist_scroll"),
        pl.col("hist_time"),
        pl.col("hist_ts")
    ])

    # 2. Explode
    q = lf_hist.explode(["hist_ids", "hist_scroll", "hist_time", "hist_ts"])
    q = q.with_columns(pl.col("hist_ids").cast(pl.Int32).alias("article_id_int"))

    # 3. Calculate Time Score (Log1p)
    # X·ª≠ l√Ω null/nan/√¢m -> log1p
    # log1p(x) = log(x + 1) -> gi√∫p th·ªùi gian 0s th√†nh 0, ph√¢n ph·ªëi m∆∞·ª£t h∆°n
    time_col = pl.col("hist_time").fill_null(0).fill_nan(0).clip(0, None)
    score_expr = time_col.log1p()

    q = q.with_columns([
        score_expr.cast(pl.Float32).alias("time_log1p"),  # ƒê·ªïi t√™n c·ªôt
        pl.col("hist_scroll").fill_null(0).cast(pl.Float32)
    ])

    # 4. Group & Slice (Tail 30)
    print("‚è≥ ƒêang gom nh√≥m v√† c·∫Øt d·ªØ li·ªáu (Taking last 30)...")
    df_grouped = (
        q.sort("hist_ts")
        .group_by("user_id")
        .agg([
            pl.col("article_id_int").tail(WPMConfig.MAX_HISTORY).alias("ids"),
            pl.col("hist_scroll").tail(WPMConfig.MAX_HISTORY).alias("scrolls"),
            pl.col("time_log1p").tail(WPMConfig.MAX_HISTORY).alias("scores")
            # L∆∞u v√†o alias scores ƒë·ªÉ d√πng l·∫°i logic d∆∞·ªõi
        ])
        .collect(streaming=False)
    )

    # 5. Fill Matrix
    max_uid = df_grouped["user_id"].max()
    if max_uid is None: max_uid = 0
    num_users = max_uid + 1

    print(f"üì¶ Creating Matrix: [{num_users}, {WPMConfig.MAX_HISTORY}]")

    mat_ids = np.zeros((num_users, WPMConfig.MAX_HISTORY), dtype=np.int32)
    mat_scr = np.zeros((num_users, WPMConfig.MAX_HISTORY), dtype=np.float32)
    mat_sco = np.zeros((num_users, WPMConfig.MAX_HISTORY), dtype=np.float32)

    uids = df_grouped["user_id"].to_numpy()
    vals_ids = df_grouped["ids"].to_list()
    vals_scr = df_grouped["scrolls"].to_list()
    vals_sco = df_grouped["scores"].to_list()

    for i, uid in enumerate(uids):
        r_ids = vals_ids[i]
        length = len(r_ids)
        if length == 0: continue

        # Pre-padding logic (ƒêi·ªÅn v√†o cu·ªëi)
        mat_ids[uid, -length:] = r_ids
        mat_scr[uid, -length:] = vals_scr[i]
        mat_sco[uid, -length:] = vals_sco[i]

    print(f"üíæ Saving cache: {output_path}")
    np.savez_compressed(
        output_path,
        matrix_ids=mat_ids,
        matrix_scrolls=mat_scr,
        matrix_scores=mat_sco  # V·∫´n gi·ªØ t√™n key file l√† matrix_scores cho ƒë·ªìng b·ªô
    )

    del df_grouped, mat_ids, mat_scr, mat_sco
    gc.collect()
    print("‚úÖ Cache built successfully!")


# ==========================================
# 2. DATASET (Reading Pre-computed Matrix)
# ==========================================
class NAMLIterableDataset(IterableDataset):
    def __init__(self, behaviors_path, cache_path,
                 neg_ratio=4, batch_size=32,
                 mode='train', shuffle_buffer_size=10000):
        super().__init__()
        self.behaviors_path = behaviors_path
        self.cache_path = cache_path
        self.neg_ratio = neg_ratio
        self.mode = mode
        self.batch_size = batch_size
        self.shuffle_buffer_size = shuffle_buffer_size if mode == 'train' else 0

        self._load_cache()

    def _load_cache(self):
        if not os.path.exists(self.cache_path):
            raise FileNotFoundError(f"‚ùå Cache missing: {self.cache_path}")

        # Map file v√†o RAM
        data = np.load(self.cache_path, mmap_mode='r')
        self.mat_ids = data['matrix_ids']
        self.mat_scr = data['matrix_scrolls']
        self.mat_sco = data['matrix_scores']  # ƒê√¢y b√¢y gi·ªù ch·ª©a log1p(time)
        self.num_users_cache = self.mat_ids.shape[0]

    def _get_user_history(self, user_id):
        if user_id >= self.num_users_cache:
            # Cold start user -> Tr·∫£ v·ªÅ m·∫£ng 0
            return (
                np.zeros(WPMConfig.MAX_HISTORY, dtype=np.int32),
                np.zeros(WPMConfig.MAX_HISTORY, dtype=np.float32),
                np.zeros(WPMConfig.MAX_HISTORY, dtype=np.float32)
            )
        return (
            self.mat_ids[user_id],
            self.mat_scr[user_id],
            self.mat_sco[user_id]
        )

    def _process_batch(self, batch):
        u_ids = batch["user_id"]
        c_cols = batch["clk_ids"]
        i_cols = batch["inv_ids"]

        for i in range(len(batch)):
            uid = u_ids[i].as_py()
            c_list = c_cols[i].as_py() or []
            i_list = i_cols[i].as_py() or []

            # Negative Sampling Logic
            if self.mode == 'train' and c_list:
                pos_id = np.random.choice(c_list)
            elif c_list:
                pos_id = c_list[0]
            elif i_list:
                pos_id = i_list[0]
            else:
                continue

            neg_pool = list(set(i_list) - set(c_list))

            if len(neg_pool) >= self.neg_ratio:
                if self.mode == 'train':
                    neg_ids = np.random.choice(neg_pool, self.neg_ratio, replace=False).tolist()
                else:
                    neg_ids = neg_pool[:self.neg_ratio]
            else:
                neg_ids = neg_pool + [0] * (self.neg_ratio - len(neg_pool))

            cand_ids = [pos_id] + neg_ids
            labels = [1.0] + [0.0] * self.neg_ratio

            # Load History from Matrix
            h_ids, h_scr, h_sco = self._get_user_history(uid)

            # Yield sample
            yield {
                "hist_indices": torch.from_numpy(h_ids).long(),
                "hist_scroll": torch.from_numpy(h_scr).float(),
                # ƒê·ªîI T√äN KEY ·ªû ƒê√ÇY CHO R√ï NGHƒ®A
                "hist_time_log1p": torch.from_numpy(h_sco).float(),
                "cand_indices": torch.tensor(cand_ids, dtype=torch.long),
                "labels": torch.tensor(labels, dtype=torch.float)
            }

    def __iter__(self):
        worker_info = get_worker_info()
        num_workers = worker_info.num_workers if worker_info else 1
        worker_id = worker_info.id if worker_info else 0
        np.random.seed(42 + worker_id)

        if dist.is_available() and dist.is_initialized():
            world_size = dist.get_world_size()
            rank = dist.get_rank()
        else:
            world_size, rank = 1, 0

        pq_file = pq.ParquetFile(self.behaviors_path)
        iter_batches = pq_file.iter_batches(batch_size=self.batch_size * 20)

        total_workers = world_size * num_workers
        global_worker_id = rank * num_workers + worker_id
        sharded_iter = islice(iter_batches, global_worker_id, None, total_workers)

        buffer = []
        for batch in sharded_iter:
            for sample in self._process_batch(batch):
                if self.shuffle_buffer_size > 0:
                    buffer.append(sample)
                    if len(buffer) >= self.shuffle_buffer_size:
                        yield buffer.pop(np.random.randint(len(buffer)))
                else:
                    yield sample

        if self.shuffle_buffer_size > 0:
            np.random.shuffle(buffer)
            yield from buffer


# ==========================================
# 3. LIGHTNING DATA MODULE
# ==========================================
class NAMLDataModule(L.LightningDataModule):
    def __init__(self, root_path, batch_size=32, neg_ratio=4, num_workers=4):
        super().__init__()
        self.save_hyperparameters()
        self.root = Path(root_path)

    def prepare_data(self):
        train_hist = self.root / "train" / "history_processed.parquet"
        val_hist = self.root / "validation" / "history_processed.parquet"
        articles = self.root / "articles_processed.parquet"

        # ƒê·ªïi t√™n file cache ƒë·ªÉ tr√°nh d√πng nh·∫ßm cache c≈© (ch·ª©a wpm_score)
        train_cache = self.root / "train_matrix_log1p.npz"
        val_cache = self.root / "val_matrix_log1p.npz"

        if not train_cache.exists():
            build_history_matrix(train_hist, articles, train_cache)

        if not val_cache.exists():
            build_history_matrix(val_hist, articles, val_cache)

    def setup(self, stage=None):
        train_beh = self.root / "train" / "behaviors_processed.parquet"
        val_beh = self.root / "validation" / "behaviors_processed.parquet"

        train_cache = self.root / "train_matrix_log1p.npz"
        val_cache = self.root / "val_matrix_log1p.npz"

        if stage == "fit" or stage is None:
            self.train_ds = NAMLIterableDataset(
                train_beh, train_cache,
                neg_ratio=self.hparams.neg_ratio,
                batch_size=self.hparams.batch_size,
                mode='train'
            )
            self.val_ds = NAMLIterableDataset(
                val_beh, val_cache,
                neg_ratio=self.hparams.neg_ratio,
                batch_size=self.hparams.batch_size,
                mode='val'
            )

    def train_dataloader(self):
        return DataLoader(self.train_ds,
                          batch_size=self.hparams.batch_size,
                          num_workers=self.hparams.num_workers,
                          pin_memory=True)

    def val_dataloader(self):
        return DataLoader(self.val_ds,
                          batch_size=self.hparams.batch_size,
                          num_workers=self.hparams.num_workers,
                          pin_memory=True)


# ==========================================
# 4. MAIN execution (TESTING)
# ==========================================
if __name__ == "__main__":
    # C·∫≠p nh·∫≠t h√†m in ƒë·ªÉ match v·ªõi key m·ªõi
    def pretty_print_sample(batch, sample_idx=0):
        h_ids = batch['hist_indices'][sample_idx].tolist()
        h_scr = batch['hist_scroll'][sample_idx].tolist()
        # Key m·ªõi: hist_time_log1p
        h_sco = batch['hist_time_log1p'][sample_idx].tolist()

        c_ids = batch['cand_indices'][sample_idx].tolist()
        lbls = batch['labels'][sample_idx].tolist()

        display_len = 10
        print(f"   üë§ [SAMPLE {sample_idx}] USER CONTEXT (Last {display_len}/{len(h_ids)} items):")
        print(f"      ORDER :  " + "  ".join([f"{i:>5}" for i in range(len(h_ids) - display_len, len(h_ids))]))
        print(f"      üÜî ID :  " + "  ".join([f"{x:>5}" for x in h_ids[-display_len:]]))
        print(f"      üìú SCR:  " + "  ".join([f"{x:>5.1f}" for x in h_scr[-display_len:]]))
        # In ti√™u ƒë·ªÅ l√† LOG1P
        print(f"      üïí LOG:  " + "  ".join([f"{x:>5.2f}" for x in h_sco[-display_len:]]))

        print(f"\n   üéØ TARGET (Candidates & Labels):")
        print(f"      üÜî CAND: " + "  ".join([f"{c:>7}" for c in c_ids]))
        print(f"      üè∑Ô∏è LABL: " + "  ".join([f"{l:>7.0f}" for l in lbls]))

    # --- CH·∫†Y TH·ª¨ ---
    # DATA_ROOT = "/path/to/your/data"
    # BATCH_SIZE = 4
    # ... (Ph·∫ßn ch·∫°y th·ª≠ gi·ªØ nguy√™n logic, ch·ªâ c·∫ßn g·ªçi ƒë√∫ng h√†m in) ...