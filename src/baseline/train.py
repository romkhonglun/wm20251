import os
import argparse
import torch
import pytorch_lightning as L
from pytorch_lightning.callbacks import ModelCheckpoint, RichProgressBar, RichModelSummary
from pytorch_lightning.loggers import WandbLogger
from dotenv import load_dotenv
from pathlib import Path

# --- IMPORT MODULES ƒê√É T·∫†O ---
# Gi·∫£ s·ª≠ b·∫°n l∆∞u c√°c class v√†o c√°c file t∆∞∆°ng ·ª©ng:
# model.py -> ch·ª©a NAMLConfig, OriginalNAML
# dataset.py -> ch·ª©a NAMLDataModule
# lightning_module.py -> ch·ª©a NAMLModule (class wrapper ƒë√£ s·ª≠a ·ªü b∆∞·ªõc tr∆∞·ªõc)

from model import NAMLConfig
from dataset import NAMLDataModule
from lightning_module import NAMLModule

# Load bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

# T·ªëi ∆∞u Threading cho CPU
torch.set_num_threads(4)
os.environ["OMP_NUM_THREADS"] = "4"
os.environ["MKL_NUM_THREADS"] = "4"

# ==========================================
# C·∫§U H√åNH M·∫∂C ƒê·ªäNH
# ==========================================
PROCESSED_DIR = "/processed_parquet"
EMBEDDING_DIR = "/embedding"



def parse_args():
    parser = argparse.ArgumentParser(description="Train NAML model")

    # Paths
    parser.add_argument("--root-dir", type=str, default=PROCESSED_DIR, help="Root path containing train/val folders")
    parser.add_argument("--embedding-dir", type=str, default=EMBEDDING_DIR,
                        help="Directory containing .npy embedding files")

    # Hyperparameters
    parser.add_argument("--batch-size", type=int, default=512, help="Batch size")
    parser.add_argument("--epochs", type=int, default=5, help="Number of epochs")
    parser.add_argument("--lr", type=float, default=1e-3, help="Learning rate")
    parser.add_argument("--weight-decay", type=float, default=1e-5, help="Weight decay")

    # Scheduler
    parser.add_argument("--lr-scheduler", type=str, choices=["onecycle", "cosine"], default="onecycle")
    parser.add_argument("--total-steps", type=int, default=10000, help="Explicit total steps for OneCycleLR")

    # Hardware
    parser.add_argument("--num-workers", type=int, default=2, help="Number of DataLoader workers")

    return parser.parse_args()


def main():
    args = parse_args()

    # Set seed
    L.seed_everything(42)

    print("=" * 40)
    print("   NAML TRAINING PIPELINE   ")
    print("=" * 40)

    # 1. Init Config
    # Kh√¥ng c·∫ßn TIME_FEATURE_NAMLConfig n·ªØa v√¨ ta ƒëang d√πng OriginalNAML chu·∫©n
    config = NAMLConfig()

    # B·∫°n c√≥ th·ªÉ override config b·∫±ng args n·∫øu mu·ªën (v√≠ d·ª• dropout)
    # config.dropout = 0.3 

    print(f"Model Config: EmbedDim={config.embedding_dim}, Filters={config.num_filters}")
    print(f"Data Dir: {args.root_dir}")
    print(f"Emb Dir:  {args.embedding_dir}")

    # 2. Init DataModule (Phi√™n b·∫£n t·ªëi ∆∞u)
    dm = NAMLDataModule(
        root_path=args.root_dir,
        embedding_path=args.embedding_dir,  # DataModule d√πng c√°i n√†y ƒë·ªÉ map ID
        batch_size=args.batch_size,
        history_len=30,  # C√≥ th·ªÉ ƒë∆∞a ra arg
        neg_ratio=4,
        num_workers=args.num_workers
    )

    # 3. T√≠nh to√°n Total Steps cho Scheduler
    # V√¨ d√πng IterableDataset, ta c·∫ßn ∆∞·ªõc l∆∞·ª£ng s·ªë b∆∞·ªõc train
    if args.total_steps is None:
        # Gi·∫£ s·ª≠ s·ªë l∆∞·ª£ng m·∫´u train (b·∫°n c√≥ th·ªÉ check file info ho·∫∑c hardcode s·ªë li·ªáu th·∫≠t)
        # V√≠ d·ª•: EB-NeRD demo ~200k samples
        ESTIMATED_SAMPLES = 200000
        steps_per_epoch = ESTIMATED_SAMPLES // args.batch_size
        calculated_total_steps = steps_per_epoch * args.epochs
        print(f"‚ÑπÔ∏è Auto-calculated total_steps: {calculated_total_steps} (Est. Samples: {ESTIMATED_SAMPLES})")
    else:
        calculated_total_steps = args.total_steps
        print(f"‚ÑπÔ∏è Using provided total_steps: {calculated_total_steps}")

    # 4. Init Lightning Module
    model = NAMLModule(
        config=config,
        embedding_dir=args.embedding_dir,  # Model d√πng c√°i n√†y ƒë·ªÉ load weight
        lr=args.lr,
        weight_decay=args.weight_decay,
        total_steps=calculated_total_steps,
        scheduler_type=args.lr_scheduler
    )

    # 5. Logger (Wandb)
    wandb_logger = WandbLogger(
        project="NAML-RecSys",
        name=f"naml-bs{args.batch_size}-lr{args.lr}",
        log_model=False,
        mode="offline"  # ƒê·ªïi th√†nh "online" khi ch·∫°y th·∫≠t
    )

    # 6. Callbacks
    checkpoint_callback = ModelCheckpoint(
        dirpath="checkpoints",
        filename="naml-{epoch:02d}-{val/auc:.4f}",
        save_top_k=3,
        monitor="val/auc",
        mode="max",
        verbose=True
    )

    # 7. Trainer
    trainer = L.Trainer(
        accelerator="auto",  # T·ª± ch·ªçn GPU/MPS/CPU
        devices="auto",
        strategy="auto",
        logger=wandb_logger,
        callbacks=[
            checkpoint_callback,
            RichModelSummary(max_depth=2),
            RichProgressBar()
        ],
        max_epochs=args.epochs,
        precision="16-mixed",  # Mixed Precision cho GPU (nhanh h∆°n, t·ªën √≠t VRAM)
        log_every_n_steps=50,
        gradient_clip_val=0.5  # Clip gradient ƒë·ªÉ ·ªïn ƒë·ªãnh training
    )

    # 8. Start Training
    print("üöÄ Starting training...")
    trainer.fit(model, datamodule=dm)

    print(f"‚úÖ Training finished. Best model path: {checkpoint_callback.best_model_path}")


if __name__ == "__main__":
    main()